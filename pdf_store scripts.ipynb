{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out what to do with errors in the pmid name\n",
    "# figure out how to handle the duplicate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow\n",
    "# files_list = Directory_list(PDF_COPY)\n",
    "# cleanup = Analyze_pdf_store(files_list.files_list, PDF_COPY)\n",
    "# pmids = cleanup.find_pmids()\n",
    "# filenames_matches = cleanup.create_filenames_digit_matches()\n",
    "# filter_results = cleanup.filter_files(filenames_matches) to see state of files and \n",
    "# categories to be changed or not\n",
    "# pmid_pubmed_data, errors = cleanup.get_pubmed_data(pmids) may take >20 min\n",
    "# pmid_pubmed_data = pd.read_csv(DATAFILES/'pmid_pubmed.csv') to load previous data\n",
    "# results = cleanup.rename_pmid_files(pmids, pmid_pubmed_data, delete=False)\n",
    "# change to delete=True to remove files, results list files altered or not\n",
    "# cleanup.remove_preceeding_spaces(filter_results['preceeding_spaces'], delete=False)\n",
    "# cleanup.move_supplementals(filter_results['supplementals'], SUPP_DEST, remove=False)\n",
    "# cleanup.rename_capitalized_extensions(check_file=True, rename=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divided-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import filecmp\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pdf_store_cleanup import Copy_files\n",
    "load_dotenv()\n",
    "Entrez.api_key = os.getenv('Entrez.api_key')\n",
    "Entrez.email = os.getenv('Entrez.email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advised-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_STORE = Path(\"/Users/dht/Google Drive File Stream/Shared drives/PDF Store/\")\n",
    "PDF_COPY = Path(\"/Users/dht/Google Drive File Stream/My Drive/files/PDF_Store_copy/\")\n",
    "PDF_COPY_MOVED = Path(PDF_COPY/'moved_files/')\n",
    "SUPP_DEST = Path(PDF_COPY/'supp_files')\n",
    "DATAFILES = Path(\"/Users/dht/datafiles/pdf_store_repo_files/\")\n",
    "TEST_FOLDER = Path(DATAFILES/\"test_folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "selected-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGITS_MATCH = re.compile('^(\\d+)')\n",
    "PARENTHESES_MATCH = re.compile('^.* \\(\\d+\\).pdf')\n",
    "UP_TO_PARENTHESES_DIGIT = re.compile('(.+?)\\(\\d+\\)')\n",
    "UP_TO_WHITESPACE = re.compile('^\\S+')\n",
    "PMID_AUTHOR_YEAR= re.compile('^[0-9]+[ ][a-zA-Z-]+[ ][0-9]+$')\n",
    "PRECEEDING_WHITESPACE = re.compile('^\\s+')\n",
    "PMID_AUTHOR_YEAR_UNDERSCORE = re.compile('^[0-9]+_[a-zA-Z- ]+_[0-9]+.pdf$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patient-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Directory_list:\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        paths = directory_path.glob(\"*\")\n",
    "        self.files_list = [f for f in paths if f.is_file()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        repr_string = (f\"\"\"\n",
    "        path to analyze {self.directory_path},\n",
    "        number of files: {len(list(self.files_list))}\n",
    "        \"\"\")\n",
    "        return repr_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wound-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyze_pdf_store:\n",
    "    \"\"\"class to sort/filter PDF directory. Goal is to convert PDF files \n",
    "    to pmid author year.pdf filename (assuming it's journal article \n",
    "    with a pubmed pmid)\n",
    "    \n",
    "    PDF files may have one or more duplicate files.  There are also \n",
    "    non-pmid PDFs, supplementary tables/docs, non-PDF files, and other files\n",
    "    \n",
    "    Args: \n",
    "        files_list = list of pathlib Paths for each file (is_file() = True)\n",
    "        directory_path = pathlib Path for PDF directory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, files_list=None, directory_path=None):\n",
    "        self.files_list = files_list\n",
    "        self.directory_path = directory_path\n",
    "               \n",
    "    def __repr__(self):\n",
    "        repr_string = (f\"\"\"\n",
    "        first five files: {self.files_list[0:4]}\n",
    "        number of files: {len(list(self.files_list))}\n",
    "        \n",
    "        directory_path: {self.directory_path}\n",
    "        \"\"\")\n",
    "        return repr_string\n",
    "    \n",
    "    def find_pmids(self, file_list=None):\n",
    "        \"\"\"\n",
    "        Function to sort through list of file paths (using pathlib.glob)\n",
    "        for duplicates.  Creates dict with putative PMID as key and all\n",
    "        files that are returned searching for that PMID (excludes any with supp in\n",
    "        name)\n",
    "\n",
    "        1) creates list of file names from list of file paths in directory\n",
    "        2) takes each pmid and looks for all filenames that contain that pmid \n",
    "            (excludes filenames containing \"supp\", case insensitive)\n",
    "        3) creates dict with keys=pmid, and values=list of all filenames with that pmid\n",
    "        4) check if files with the same pmid are duplicates by checking file sizes\n",
    "            flag those pmids/files that do not \n",
    "        5) return list of pmid\n",
    "        \n",
    "        Arg: file_list is list of filenames (not paths)\n",
    "\n",
    "        \"\"\"\n",
    "        pmids = []\n",
    "        supp_data_filters = ['supp', 'table']\n",
    "        if file_list is None:\n",
    "            for file_ in self.files_list:\n",
    "                pmid = None\n",
    "                not_supp = any(file_.stem.lower() not in filter_ \n",
    "                               for filter_ in supp_data_filters)\n",
    "                if not_supp:\n",
    "                    pmid_match = re.match(DIGITS_MATCH, file_.stem)\n",
    "                    if pmid_match:\n",
    "                        pmid = pmid_match.group(0)\n",
    "                    if pmid and pmid not in pmids:\n",
    "                        pmids.append(pmid)\n",
    "        else:\n",
    "            #assumes is list of file names and not paths\n",
    "            for file in file_list:\n",
    "                pmid = None\n",
    "                name = file.split(\".\")[0]\n",
    "                not_supp = any(name.lower() not in filter_ \n",
    "                               for filter_ in supp_data_filters)\n",
    "                if not_supp:\n",
    "                    pmid_match = re.match(DIGITS_MATCH, name)\n",
    "                    if pmid_match:\n",
    "                        pmid = pmid_match.group(0)\n",
    "                    if pmid and (pmid not in pmids):\n",
    "                        pmids.append(pmid)\n",
    "        return sorted(pmids)\n",
    "\n",
    "    def match_digits(self, name):\n",
    "        \"\"\"matches first string of digits in string and returns, \n",
    "        returns None if no match\n",
    "        \"\"\"\n",
    "        first_digits_match = re.match(DIGITS_MATCH, name)\n",
    "        if first_digits_match:\n",
    "            first_digits = first_digits_match.group(0)\n",
    "            return first_digits\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def create_filenames_digit_matches(self):\n",
    "        \"\"\"create a list of tuples from the list of file\n",
    "        paths in self.files_list where:\n",
    "        \n",
    "        \n",
    "        first item = regex extract of preceeding digits in name\n",
    "        second item = Path.stem\n",
    "        (if one exists, i.e. pmid number)\n",
    "        \n",
    "        (filename stems, DIGITS_MATCH regex extract)\n",
    "        \n",
    "        The list of tuples is used to facilitate filtering \n",
    "        of files\n",
    "        \"\"\" \n",
    "        regex_matches = []\n",
    "        filenames = [filename.name for filename in self.files_list]\n",
    "        for name in filenames:\n",
    "            regex_match = self.match_digits(name)\n",
    "            regex_matches.append(regex_match)\n",
    "        names_matches_tuple = list(zip(regex_matches, filenames))\n",
    "        return names_matches_tuple\n",
    "                   \n",
    "    def filter_files(self, filenames_matches):\n",
    "        \"\"\"\n",
    "        Takes tuples of (digit_match, filename) where\n",
    "        digits_match = first string of digits if one exists \n",
    "        filename = filename.ext\n",
    "        \n",
    "        Sorts into different lists bases on criteria:\n",
    "        \n",
    "        supplementals = 'supp' and 'table' in name or 'ppt',\n",
    "        'png', 'docx' in name \n",
    "        \n",
    "        digits_match is None followed by: \n",
    "        - preceeding_spaces = filename has one or more spaces\n",
    "        at the start\n",
    "        - no_pmids = no preceeding digits in name\n",
    "        - pmid_in_name = 'pmid' in filename \n",
    "        \n",
    "        correct_name = pmid author year\n",
    "        others = remaining files\n",
    "        \n",
    "        Returns a dict with keys corresponding to above \n",
    "        categories\n",
    "        \"\"\"     \n",
    "        index = 0\n",
    "        supp_data_filters = ['supp', 'table']\n",
    "        suffix_filters = ['ppt', 'png', 'docx']\n",
    "        filter_results = {}\n",
    "        supplementals = []\n",
    "        pmid_pdf = []\n",
    "        pmid_in_name = []\n",
    "        no_pmids = []\n",
    "        others = []\n",
    "        correct_name = []\n",
    "        preceeding_spaces = []\n",
    "        for digits_match, filename in filenames_matches:\n",
    "            index += 1\n",
    "            # skip the Mac Icon directory file\n",
    "            if filename == 'Icon\\r':\n",
    "                continue\n",
    "            try: \n",
    "                filename_stem = filename.split(\".\")[0]\n",
    "                filename_suffix = filename.split(\".\")[1]\n",
    "            except IndexError:\n",
    "                print(filename)\n",
    "            supp_filter_flag = any(supp_filter in filename.lower() \n",
    "                                   for supp_filter in supp_data_filters) \n",
    "            suffix_filter_flag = any(suffix_filter in filename.lower() \n",
    "                                    for suffix_filter in suffix_filters) \n",
    "            # finds supplemental data files or file extensions in suffix_filters\n",
    "            if supp_filter_flag or suffix_filter_flag:\n",
    "                supplementals.append(filename)\n",
    "            # finds files with no preceeding digits, preceeding spaces\n",
    "            elif digits_match is None:\n",
    "                if re.match(PRECEEDING_WHITESPACE, filename):\n",
    "                    preceeding_spaces.append(filename)\n",
    "                elif 'pmid' in filename.lower():\n",
    "                    pmid_in_name.append(filename)\n",
    "                else:\n",
    "                    no_pmids.append(filename)\n",
    "            elif filename == (digits_match + \".pdf\"):\n",
    "                pmid_pdf.append(filename)\n",
    "            elif re.match(PMID_AUTHOR_YEAR, filename_stem):\n",
    "                correct_name.append(filename)\n",
    "            else:\n",
    "                others.append(filename)\n",
    "        filter_results['supplementals'] = supplementals\n",
    "        filter_results['no_pmids'] = no_pmids\n",
    "        filter_results['pmid_pdf'] = pmid_pdf\n",
    "        filter_results['preceeding_spaces'] = preceeding_spaces\n",
    "        filter_results['others'] = others\n",
    "        filter_results['pmid_in_name'] = pmid_in_name\n",
    "        filter_results['correct_name'] = correct_name\n",
    "        return filter_results\n",
    "            \n",
    "    def get_pubmed_data(self, pmids):\n",
    "        \"\"\"query Pubmed with Biopython's Entrez module\n",
    "        extract first author (from AuthorList with regex match\n",
    "        of first characters up to whitespace; ideally will get \n",
    "        non-English characters), date, and journal names.  \n",
    "\n",
    "        Save to dict\n",
    "        \"\"\"\n",
    "        pmid_pubmed_data = {}\n",
    "        errors = []\n",
    "        for pmid in pmids:\n",
    "            single_pmid_data = {}\n",
    "            try:\n",
    "                record = Entrez.read(Entrez.esummary(db=\"pubmed\", id=pmid)) \n",
    "            except RuntimeError:\n",
    "                errors.append(pmid)\n",
    "                continue\n",
    "            author_list = record[0].get('AuthorList')\n",
    "            if author_list:\n",
    "                first_characters = re.match(UP_TO_WHITESPACE, author_list[0])\n",
    "                if first_characters:\n",
    "                    first_author = first_characters[0]\n",
    "            date = record[0].get('PubDate')\n",
    "            journal = record[0].get('FullJournalName')\n",
    "            new_filename = pmid+\" \"+first_author+\" \"+date[0:4]\n",
    "            single_pmid_data['first_author'] = first_author\n",
    "            single_pmid_data['date'] = date\n",
    "            single_pmid_data['journal'] = journal\n",
    "            single_pmid_data['new_filename'] = new_filename\n",
    "            pmid_pubmed_data[pmid] = single_pmid_data\n",
    "        return pmid_pubmed_data, errors\n",
    "\n",
    "    def rename_pmid_files(self, pmids, pubmed_data, delete=False, pmid_space_pdf=False):\n",
    "        \"\"\"take pmid.pdf and renames to pmid author year.pdf if not a\n",
    "        duplicate\n",
    "        \n",
    "        Args: \n",
    "        pmids = list of pmids\n",
    "        pubmed_data = pandas dataframe with pmid, date, journal, new_filename\n",
    "    \n",
    "        \"\"\"\n",
    "        def get_size(file):\n",
    "            file = Path(file)\n",
    "            return file.stat().st_size\n",
    "        \n",
    "        results = {}\n",
    "        correct_files = []\n",
    "        leading_zeros = []\n",
    "        not_found_pubmed = []\n",
    "        renamed = []\n",
    "        for pmid in pmids:\n",
    "            correct_filename, correct_file, pmid_pdf = None, None, None\n",
    "            # avoid converting 00524 to 524 for example\n",
    "            if pmid[0] != '0':\n",
    "                pmid_int = int(pmid)\n",
    "            else: \n",
    "                print(f\"leading 0 for {pmid}\")\n",
    "                leading_zeros.append(pmid)\n",
    "                continue\n",
    "            try:\n",
    "                author_date_year = (\n",
    "                    pubmed_data.loc[pubmed_data['pmid'] == pmid_int, 'new_filename'].values[0]\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"error looking up pmid in dataframe {pmid}\")\n",
    "                not_found_pubmed.append(pmid)\n",
    "                continue\n",
    "            if pmid_space_pdf: \n",
    "                pmid_pdf = pmid + \" .pdf\"\n",
    "            else:\n",
    "                pmid_pdf = pmid + \".pdf\"\n",
    "            pmid_pdf_path = Path(self.directory_path/pmid_pdf)\n",
    "            correct_filename = author_date_year+\".pdf\"\n",
    "            correct_file_path = Path(self.directory_path/correct_filename)\n",
    "            correct_file_exists = correct_file_path.is_file()\n",
    "            pmid_pdf_exists = pmid_pdf_path.is_file()\n",
    "            if correct_file_exists:\n",
    "                correct_file_size = get_size(correct_file_path)\n",
    "                print(f\"correct file {correct_file_path.name} exists\")\n",
    "                correct_files_tuple = (correct_filename, correct_file_size)\n",
    "                correct_files.append(correct_files_tuple)\n",
    "                if delete:\n",
    "                    print(f\"deleting {pmid_pdf_path}\")\n",
    "                    pmid_pdf_path.unlink()\n",
    "            elif pmid_pdf_exists and not correct_file_exists:\n",
    "                print(f\"rename {pmid_pdf_path} to {correct_file_path}\")\n",
    "                pmid_pdf_path.rename(correct_file_path)\n",
    "                renamed_tuple = (pmid_pdf, correct_filename)\n",
    "                renamed.append(renamed_tuple)\n",
    "        results['correct_files'] = correct_files\n",
    "        results['renamed'] = renamed\n",
    "        results['not_found_pubmed'] = not_found_pubmed\n",
    "        results['leading_zeros'] = leading_zeros\n",
    "        return results   \n",
    "    \n",
    "    def rename_capitalized_extensions(self, check_file=True, rename=False):\n",
    "        \"\"\"rename capitalized PDF to pdf\n",
    "        \"\"\"\n",
    "        paths = self.directory_path.glob(\"*\")\n",
    "        files_list = [f for f in paths if f.is_file()]\n",
    "        for index,file in enumerate(files_list):\n",
    "            if file.suffix == '.PDF':\n",
    "                if check_file:\n",
    "                    print(file)\n",
    "                    new_file = f\"{file.parent}{file.stem}'.pdf'\"\n",
    "                    print(new_file)\n",
    "                elif rename:\n",
    "                    new_file = f\"{file.parent}{file.stem}'.pdf'\"\n",
    "                    new_path = Path(new_file)\n",
    "                    print(f\"{file} renaming to .pdf \")\n",
    "                    file.rename(file.with_suffix('.pdf'))\n",
    "                    file_exists = file.is_file()\n",
    "                    print(f\"old file {file} exists = {file_exists}\")\n",
    "                    new_file_exists = new_path.is_file()\n",
    "                    print(f\"new file {new_file} exists {new_file_exists}\")\n",
    "    \n",
    "    def move_supplementals(self, supp_list, dest_path, copy=False, remove=False):\n",
    "        \"\"\"moves supplemental files (files with 'supp', 'table' in name or \n",
    "        'ppt', 'png', or 'docx') to supp_folder. Goal is to remove files \n",
    "        and facilitate further sorting\n",
    "        \n",
    "        Args: list of supplemental file names (see filter_files method)\n",
    "        \n",
    "        \"\"\"\n",
    "        source = self.directory_path\n",
    "        for file in supp_list: \n",
    "            source = Path(self.directory_path/file)\n",
    "            destination = Path(dest_path/file)\n",
    "            if copy:\n",
    "                print(f\"copying {source} to {destination}\")\n",
    "                Analyze_pdf_store.copy_file(source, destination)\n",
    "            if remove: \n",
    "                if source.is_file():\n",
    "                    print(f\"removing {source}\")\n",
    "                    print(f\"{source} exists: {source.is_file()}\")\n",
    "                    source.unlink()\n",
    "                    print(f\"{source} exists: {source.is_file()}\")\n",
    "                    print(\"\\n\")\n",
    "    \n",
    "    def remove_preceeding_spaces(self, files, delete=False):\n",
    "        \"\"\"takes files with preceeding spaces before pmid (e.g. __13431.pdf)\n",
    "        1) check if removing preceeding spaces and subsequent file already \n",
    "        exists\n",
    "        2) if not then renames file\n",
    "        3) if yes then deletes file\n",
    "        \n",
    "        Args: files = list of files (not paths)\n",
    "        \"\"\"\n",
    "        for file in files: \n",
    "            old_file = Path(self.directory_path/file)\n",
    "            file_stem = file.split(\".\")[0]\n",
    "            no_whitespace = file_stem.strip()\n",
    "            correct_name = no_whitespace+\".pdf\"\n",
    "            correct_path = Path(self.directory_path/correct_name)\n",
    "            if delete:\n",
    "                if correct_path.is_file():\n",
    "                    # check for existing file of same name\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"old path {old_file}\")\n",
    "                    print(f\"renamed: {correct_path}\")\n",
    "                    old_file.rename(correct_path)\n",
    "                    # old_file.unlink()\n",
    "    \n",
    "    def convert_underscore_names(self, files, delete=False):\n",
    "        \"\"\"convert pmid_author_year_ to correct format\n",
    "        (pmid author year.pdf)\n",
    "        \n",
    "        Arg: files = list of files\n",
    "             delete = bool, where True means Path.unlink() old \n",
    "             file\n",
    "        \"\"\"\n",
    "        for file in files: \n",
    "            if re.match(PMID_AUTHOR_YEAR_UNDERSCORE, file):\n",
    "                old_file = Path(self.directory_path/file)\n",
    "                file_stem = file.split(\".\")[0]\n",
    "                file_stem = file_stem.replace('_', ' ')\n",
    "                correct_name = file_stem+\".pdf\"\n",
    "                correct_path = Path(self.directory_path/correct_name)\n",
    "                if correct_path.is_file():\n",
    "                    # check for existing file of same name\n",
    "                    print(f\"file exists {correct_path}\")\n",
    "                    if delete and old_file.is_file():\n",
    "                        print(f\"deleting {old_file}\")\n",
    "                        old_file.unlink()\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"old path {old_file}\")\n",
    "                    print(f\"renamed: {correct_path}\")\n",
    "                    old_file.rename(correct_path)         \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "    def remove_dups_with_parentheses(self, files, delete=False):\n",
    "        \"\"\"removes files with (digit).pdf at the end, eg.\n",
    "        16339096 (1).pdf\n",
    "        \"\"\"\n",
    "        for file in files: \n",
    "            if re.match(PARENTHESES_MATCH, file):\n",
    "                old_file = Path(self.directory_path/file)\n",
    "                if delete and old_file.is_file():\n",
    "                    print(f\"deleting {old_file}\")\n",
    "                    old_file.unlink()\n",
    "                    continue\n",
    "                    \n",
    "    @staticmethod\n",
    "    def copy_file(source_file, dest_file):\n",
    "        \"\"\"uses shutil.copyfile to\n",
    "        copy source_file to dest_file\n",
    "        \"\"\"\n",
    "        print(f\"copying {source_file} to {dest_file}\")\n",
    "        shutil.copyfile(source_file, dest_file)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "potential-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = Directory_list(PDF_COPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "final-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = Analyze_pdf_store(files_list.files_list, PDF_COPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sapphire-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = cleanup.find_pmids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "animated-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_matches = cleanup.create_filenames_digit_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affecting-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_results = cleanup.filter_files(filenames_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fancy-selection",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lim 2019.pdf',\n",
       " 'Aizawa 2019.pdf',\n",
       " 'Kiesling 2014.pdf',\n",
       " 'Jiménez-Jáimez.pdf',\n",
       " 'burns 2019.pdf',\n",
       " 'Hernandez 2016.pdf',\n",
       " 'Kurian_2017.pdf',\n",
       " 'Genome-wide polygenic scores for common diseases identify individuals with risk equivalent to monogenic mutations.pdf',\n",
       " 'Kathiravel 2014.pdf',\n",
       " 'journal.pone.0170613.pdf',\n",
       " 'Yang 2018 (in Chinese - revisit).pdf',\n",
       " 'Ikegami 2020.pdf',\n",
       " 'Beffagna 2006.pdf',\n",
       " 'Lorenzon 2008.pdf',\n",
       " 'Coronaviruses-methods and protocols.pdf',\n",
       " 'Luo 2019.pdf',\n",
       " 'UMD MLH1 c.454-13A>G.pdf',\n",
       " 'ARUP MLH1 V506A.pdf',\n",
       " 'MSH6 Arg1242Ser acmg2016.pdf',\n",
       " 'MLH1 Leu749Pro Lopez 2019.pdf',\n",
       " 'Salehi 2009.pdf',\n",
       " 'Satyanarayana.pdf',\n",
       " 'Stoll 2020.pdf',\n",
       " 'Broeke 2018 thesis .pdf',\n",
       " 'Luo 2020.pdf',\n",
       " 'Sasihuseyinoglu 2018.pdf',\n",
       " 'Ramasubramanian dissertation BRCA1.pdf',\n",
       " 'Lovejoy 2018.pdf',\n",
       " 'Seow 2020.pdf',\n",
       " 'Arias-Blanco 2015.pdf',\n",
       " 'Matejcic 2020 .pdf',\n",
       " 'Lowstuter 2017.pdf',\n",
       " 'Crehalet 2012.pdf',\n",
       " 'Bonaventura2018.pdf',\n",
       " 'Posafalvi 2015.pdf',\n",
       " 'Rath 2018.pdf',\n",
       " 'Shao 2019.pdf',\n",
       " 'Wang 2010.pdf',\n",
       " 'Duong 2017.pdf',\n",
       " 'Mizusawa 2016 .pdf',\n",
       " 'alm-38-54.pdf',\n",
       " 'Science Lee 2019.pdf',\n",
       " 'Chong_H_ASHG_2016_Reclassifying_Duplication_Variants_in_High_Risk_Cancer_Genes_by_Identifying_Tandem_Duplication_Breakpoints_Poster.pdf',\n",
       " 'Aisha 2015.pdf',\n",
       " 'Mizusawa 2016.pdf',\n",
       " 'Madan 2018 (1).pdf',\n",
       " 'Burns 2019 (1).pdf',\n",
       " 'Kassem 2017 (1).pdf',\n",
       " 'Waldmuller P2062.pdf',\n",
       " 'Futema 2014.pdf',\n",
       " 'Genetic analyses of diverse populations improves discovery for complex traits.pdf',\n",
       " 'Walsh 2017.pdf',\n",
       " 'Kassem 2017.pdf',\n",
       " 'Amunugama 2013.pdf',\n",
       " 'Dott 2010.pdf',\n",
       " 'Madan 2018.pdf',\n",
       " 'ASHG 2017 (MCK RNA study reference).pdf',\n",
       " 'Schymanski 2017 (2).pdf',\n",
       " 'Schymanski 2017 (1).pdf',\n",
       " 'Rodríguez-García 2011.pdf',\n",
       " 'Kuliev 2016.pdf',\n",
       " 'Huijgen 2017.pdf',\n",
       " 'Abdelsayed 2016.pdf',\n",
       " 'Pacheco 2013.pdf',\n",
       " 'Buscemi 2015.pdf',\n",
       " 'Nicoletta LMNA .pdf',\n",
       " 'Pottinger 2019.pdf',\n",
       " 'Grishin 2019.pdf',\n",
       " 'Genetics of hypertrophic cardiomyopathy in.pdf',\n",
       " 'van Velzen, 2018.pdf',\n",
       " 'Han 2010.pdf',\n",
       " 'Walsh 2018.pdf',\n",
       " 'Fish 2010.pdf',\n",
       " 'Marschall 2019.pdf',\n",
       " 'Maryam Fish 2010 thesis.pdf',\n",
       " 'Jiménez-Jáimez 2016.pdf',\n",
       " 'Lopes 2015.pdf',\n",
       " 'Furqan 2015.pdf',\n",
       " 'Tan 2018.pdf',\n",
       " 'fodstad 2005.pdf',\n",
       " 'burns_ce_thesis.pdf',\n",
       " 'Magyar 2011.pdf',\n",
       " 'Dheeraj 2018.pdf',\n",
       " 'DOI 10.1016:j.ophtha.2019.11.009 Abdel-Rahman 2019.pdf',\n",
       " 'Lin 2019 SCN5A R1309H .pdf',\n",
       " 'Mizusawa thesis 2016 .pdf',\n",
       " 'Weerakkody thesis 2017.pdf',\n",
       " 'Schymanski 2017.pdf',\n",
       " 'Botto 2011.pdf',\n",
       " 'Solano 2018.pdf',\n",
       " 'Prevalence of Variant Reclassification Following Hereditary Cancer Genetic Testing.pdf',\n",
       " 'Making Sense of the Genome Remains a Work in Progress.pdf',\n",
       " 'Benyahya 2010.pdf',\n",
       " 'Gibbs 2018.pdf',\n",
       " 'Adherence to Medication.pdf',\n",
       " 'Ai 2016.pdf',\n",
       " 'Influence of Cardiovascular Risk Communication Tools and Presentation Formats on Patient Perceptions and Preferences.pdf',\n",
       " 'Biparental Inheritance of Mitochondrial DNA in Humans.pdf',\n",
       " 'Mundt ACMG 2017 Poster Myriad.pdf',\n",
       " 'Gerull 2018 MYH7 R1677C.pdf',\n",
       " 'Integrating behavioural health tracking in human genetics research.pdf',\n",
       " 'Learning one’s genetic risk changes physiology independent of actual genetic risk.pdf',\n",
       " 'Robertson 2015.pdf',\n",
       " 'Ohno 2013 KCNQ1 exon 16 deletion.pdf',\n",
       " 'Repurposing large health insurance claims data to estimate genetic and environmental contributions in 560 phenotypes.pdf',\n",
       " 'Borden2018.pdf',\n",
       " 'Thanumalayan 2015.pdf',\n",
       " 'US Prevalence Counts 2015.pdf',\n",
       " 'In Search of Patients With Elevated Lp(a).pdf',\n",
       " 'CHEK2 mutation in LA pop.pdf',\n",
       " 'PALB2 mutation small cell lung cancer.pdf',\n",
       " 'Dejea et al_Science_2018.pdf',\n",
       " 'PALB2 Romanian pop.pdf',\n",
       " 'Universal Panel Testing of Pancreatic Cancer Cases for Cancer Predisposition.pdf',\n",
       " 'Bastarache Science 2018.pdf',\n",
       " 'Shah 2018.pdf',\n",
       " 'jco.2017.74.1173.pdf',\n",
       " 'The Effect of CHEK2 Variant I157T on Ca...ibility- Evidence from a Meta-Analysis copy.pdf',\n",
       " 'Identification of rare de novo epigenetic variations in congenital disorders.pdf',\n",
       " 'Recent developments in genetics and medically assisted reproduction- from research to clinical applications.pdf',\n",
       " 'NSGC 2015_BRIP1_Presented by San Roman_23OCT2015_CORRECTED.pdf',\n",
       " 'The Path to Routine Genomic Screening in Health Care.pdf',\n",
       " 'A Model for Genome-First Care: Returning Secondary Genomic Findings to Participants and Their Healthcare Providers in a Large Research Cohort.pdf',\n",
       " 'VHL-Handbook.pdf',\n",
       " 'NCCN Neuroendocrine Tumors V2.2017.pdf',\n",
       " 'NCCN Thyroid Carcinoma V1.2017.pdf',\n",
       " 'JCO Precison Oncology Prostate 2017.pdf',\n",
       " 'Kuchenbaecker_2017.pdf',\n",
       " 'AHA_restrictive Cardiomyopathy.pdf',\n",
       " 'BRCA2 c.6842-2A>G ASHG poster.pdf',\n",
       " 'Italian FH study dissertation by D’Agostino.pdf',\n",
       " 'Buleje 2017.pdf',\n",
       " 'MLH1 Phe626Leu Myriad 2015 ACMG.pdf',\n",
       " 'acmg2017.04b0185.NORMAL.pdf',\n",
       " 'Barriers to PCSK9 inhibitors_GC Journal Club 09-19-17.pdf',\n",
       " 'jamaoncology_Couch_2017_oi_170012 (1).pdf',\n",
       " 'Long JAMA Onc.pdf',\n",
       " 'Breast and Ovarian Cancer Penetrance Estimates Derived From Germline Multiple-Gene Sequencing Results in Women.pdf',\n",
       " 'Lactose intolerance- diagnosis, genetic, and clinical factors.pdf',\n",
       " 'Genetics of Alcohol Metabolism Chaper Ramchandani 2013.pdf',\n",
       " 'PALB2 Bleuyard 2017.pdf',\n",
       " 'book ARVC variants.pdf',\n",
       " 'Nagel 2003 LDLR Germany.pdf',\n",
       " 'OOD_WNT10A_GD_Sept_11.pdf',\n",
       " 'US20080038723.pdf',\n",
       " 'nejmoa1603144.pdf',\n",
       " 'npjgenmed20153.pdf',\n",
       " 'GeneDx poster CDH1 E781D.pdf',\n",
       " 'Varret 2012_LDLR FH severity_Intech book chapter.pdf',\n",
       " 'Syeda_Upadhyay_2017.pdf',\n",
       " 'Childhood initiated statin therapy in familial hypercholesterolemia.pdf',\n",
       " 'batzios2009.pdf',\n",
       " 'hindawi_927352.pdf',\n",
       " 'MSH2exons1-6duplication_ASHGposter.pdf',\n",
       " 'ASHG 2015 Myriad CDH1 715G>A MLH1 306G>T.pdf',\n",
       " 'AdvExpMedBiol-v656-APC Proteins.pdf',\n",
       " \"O'Leary et al 2014.pdf\",\n",
       " 'Copy of 21378382.pdf',\n",
       " 'MSH2 EX1_7inv Ambry.pdf',\n",
       " 'CHEK2 Ambry intronic low AF.pdf',\n",
       " 'Thesis-Spearman AD-OhioState-2008.pdf',\n",
       " 'rare_variant_study_designs.pdf',\n",
       " 'Jenson et al 2013.pdf',\n",
       " 'Leenen thesis.pdf',\n",
       " 'TP53 P151T UPENN grant.pdf',\n",
       " 'TP53 P151T VAHTERISTO thesis.pdf',\n",
       " 'Lopus and Rajasekaran 2014.pdf',\n",
       " 'Sawyer, S et al JCO Oct 2012.pdf',\n",
       " 'Ambry PALB2 poster.pdf',\n",
       " 'PMC 2586153.pdf',\n",
       " 'R Chan comments on HDR assay.pdf',\n",
       " 'PIIS001650850502055X.pdf',\n",
       " 'ADA613322.pdf',\n",
       " 'MT Pyne_Thesis_University of Utah_2000.pdf',\n",
       " 'Myriad on variant ID 1741 BRCA2 9501+3A>T.pdf',\n",
       " 'MMR-BSJune09.pdf',\n",
       " 'Thesis_G_Agenbag_Univ_Stellenbosch_2005.pdf',\n",
       " 'CDH1 ortholog alignment.pdf',\n",
       " 'Detection of BRCA2 exon 10 genetic variations in Iraqi breast cancer patients.pdf',\n",
       " 'breast cancer facts and figure 2013-2014.pdf',\n",
       " 'thesis PTEN delCTTTT.pdf',\n",
       " 'Karppinen 2009 dissertation.pdf',\n",
       " 'c.7788+8G>T.pdf',\n",
       " 'c.3994-2A>G Poster.pdf',\n",
       " 'Ten new ATM alterations in Polish patients with ataxia-telangiectasia..pdf',\n",
       " 'ATM_1179delGG.pdf',\n",
       " 'Screening for ATM Mutations in an African-American Population to Identify a Predictor of Breast Cancer Susceptibility.pdf',\n",
       " 'PMC2819165.pdf',\n",
       " 'Next Generation Sequencing Approaches to Identify Novel Susceptibility Genes for Epithelial Ovarian Cancer.pdf',\n",
       " 'The role of MSH6 mutations in North American patients receiving clinical genetic testing for hereditary nonpolyposis colorectal cancer.pdf',\n",
       " 'Andersen thesis 2007.pdf',\n",
       " 'Bleeker thesis.pdf',\n",
       " 'Hayward thesis.pdf',\n",
       " 'Peter thesis.pdf',\n",
       " 'ng.3242.pdf',\n",
       " 'Harland et al 2014.pdf',\n",
       " 'Spurrell thesis .pdf',\n",
       " 'Thompson_Dissertation_2014.pdf',\n",
       " 'The role of BACH1, BARD1 and TOPBP1 genes in familial breast cancer.pdf',\n",
       " 'joi140066.pdf',\n",
       " 'H. Kim poster.pdf',\n",
       " 'Science-2014-Kaiser-687-9.pdf',\n",
       " 'genetic_disease_research_review.pdf',\n",
       " 'MLPA general information.pdf',\n",
       " 'basic_epidemiology.pdf',\n",
       " 'journal.pgen.1002998.s004.pdf',\n",
       " 'susceptibility genes in hereditary breast cancers.pdf',\n",
       " 'NSGC 2013 BARD1.pdf',\n",
       " 'CancerEpidemiology.pdf',\n",
       " 'HDGC_ASCO_JamesMFord.pdf',\n",
       " 'proefschrift rogierdef.pdf',\n",
       " 'Introduction to Genetic Association Studies.pdf',\n",
       " 'ATM-02AhmadAtaxiaTachibana.pdf',\n",
       " 'jmedgene_42_8_633_v2_1.pdf',\n",
       " 'MedicalBiostatistics.pdf',\n",
       " 'journal.pone.0025531.s001.pdf',\n",
       " 'Heidi Rehm-patterson_1113.pdf',\n",
       " 'GENEDX_info_sheet_Comprehensive_cancer_panel.pdf',\n",
       " 'Clinical Genome Sequencing-2013.pdf',\n",
       " 'Kalb_Dissertation.pdf',\n",
       " 'Invitae Analytic Validation Whitepaper.pdf',\n",
       " 'PPM1D-978-951-44-7162-9.pdf',\n",
       " 'consent-forms-pdf.pdf',\n",
       " 'Molecular Testing Consent.pdf',\n",
       " 'Mayo-Informed_Consent_for_Genetic_Testing_mc1235-117.pdf',\n",
       " 'AMBRY-Cancer_ConsentForm03-29-14.pdf',\n",
       " 'EMORY-Molecular Testing Consent (1).pdf',\n",
       " 'Invitae_patient_consent_form.pdf',\n",
       " 'Invitae_Sample_Report.pdf',\n",
       " 'GeneDx-Oncology_Requisition.pdf',\n",
       " 'MSH2-MLH1-Test.pdf',\n",
       " 'BRCA2-JNCI J Natl Cancer Inst-1999--1310-6.pdf',\n",
       " 'Illumina_datasheet_genomic_sequence.pdf',\n",
       " 'S.Newman_Dspace.pdf',\n",
       " 'Classification Matrix - PDF.pdf',\n",
       " 'classification for clia.pdf',\n",
       " 'Color Classification Matrix - PDF.pdf',\n",
       " 'BRCA-MAYO-doc-10026644.pdf',\n",
       " 'ATM Poster-ashg.pdf',\n",
       " 'ADA544966.pdf',\n",
       " 'ISSN_0970-4140.pdf',\n",
       " 'BRCA1-7-26-1-PB.pdf',\n",
       " 'BRCA-doc-10026644.pdf',\n",
       " 'MMR-CLASSIFICATION-CRITERIA.pdf',\n",
       " 'journal.pone.0012260.s005.pdf',\n",
       " 'BARD1-c.1977A>G.pdf',\n",
       " 'ADA335004.pdf',\n",
       " 'mendeldiseases.pdf',\n",
       " 'MedicalGenomics-1-s2.0-S2212066114000155-main.pdf',\n",
       " 'ClinGen-Jonathan Berg.pdf',\n",
       " 'ACMG Segregation Analysis Eggington Presented 22MAR2013 (2).pdf',\n",
       " 'Lynch-IHC-ICC-recommendations.pdf',\n",
       " 'CMGS-VUS-Guidelines.pdf',\n",
       " 'likelihood-ratios.pdf',\n",
       " 'splicing-variants-12051922401407.pdf',\n",
       " 'ACGS-guidelines-evaluation_and_reporting_of_sequence_variants_bpgs_june_2013_-_finalpdf.pdf',\n",
       " 'AMBRY-HereditaryCancerPanelsWhitePaper_0814_web_2.pdf',\n",
       " 'ACMG-ISTH SSC Bellissimo Thrombogenomics.pdf',\n",
       " 'myRisk-Clinical-Handbook-web.pdf',\n",
       " 'VarClass-1-s2.0-S2212066114000155-main.pdf',\n",
       " 'LMM_VariantClassification_05.26.11.pdf',\n",
       " 'variant_classification-openhelix.pdf',\n",
       " 'WALSH-Using_the_Bravo_Liquid-Handling_System_for_Next_Generation_Sequencing_Sample_Prep.pdf',\n",
       " 'Overview of Interpretation of Sequence Variants_handout.pdf',\n",
       " 'MSH6-PMS2.pdf',\n",
       " 'Dolman 2013 thesis.pdf',\n",
       " 'Ioannidis_Venice_Criteria.pdf',\n",
       " 'Chittenden 2010 poster.pdf',\n",
       " 'Santhiya-2013-Computational.pdf',\n",
       " 'Ambry BARD1 Arg659.pdf',\n",
       " 'Kurian 2017 ascopubs.org.pdf',\n",
       " 'Maeda 2009.pdf',\n",
       " 'Murakami 2014.pdf',\n",
       " 'Geisinger study, Buchanan et al.pdf',\n",
       " 'Meulen Erasmus University.pdf',\n",
       " 'Moller 2008.pdf',\n",
       " 'Ren 2020 .pdf',\n",
       " 'Hartgers 2020 thesis.pdf',\n",
       " 'Callis thesis LDLR N407K 1248C>G.pdf',\n",
       " 'Hartgers 2020 thesis (1).pdf',\n",
       " 'Fischer 2019 dissertation TP53.pdf',\n",
       " 'van Waning 2020.pdf',\n",
       " 'Herbst 2020 dissertation.pdf',\n",
       " 'Riele 2016.pdf',\n",
       " 's41436-020-0926-y.pdf',\n",
       " 'Hartgers thesis 2020.pdf',\n",
       " 'Riedlinger 2020.pdf',\n",
       " 'Krishnan 2016 10.1158:1538-7445.AM2016-4493.pdf',\n",
       " 'Potjer 2019 thesis.pdf',\n",
       " 'Germline Mutation Data IARC TP53 Database, R17.xlsx',\n",
       " 'UMD TP53 germline database.xlsx',\n",
       " 'Homo sapiens chromosome 17, GRCh38.p2 Primary Assembly - Nucleotide - NCBI.html',\n",
       " 'Harland et al 2014 add file 2.xlsx',\n",
       " 'CHEK2-S0002929707627049.html',\n",
       " 'BRCA-S0002929707631097.html',\n",
       " 'BRIP1-S009286740100304X.html',\n",
       " 'contentLoader.js']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results['no_pmids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "planned-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_STORE_analysis = pd.DataFrame.from_dict(filter_results, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "motivated-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_STORE_analysis = PDF_STORE_analysis.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "senior-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_STORE_analysis = PDF_STORE_analysis.drop(['pmid_in_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rural-boring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supplementals</th>\n",
       "      <th>no_pmids</th>\n",
       "      <th>pmid_pdf</th>\n",
       "      <th>preceeding_spaces</th>\n",
       "      <th>others</th>\n",
       "      <th>correct_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24123850 supp.docx</td>\n",
       "      <td>Lim 2019.pdf</td>\n",
       "      <td>29365890.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>25691505 related PhD thesis with carrier pedig...</td>\n",
       "      <td>31894144 McKinney 2020.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25590978_Inra_2015 supplemental.doc</td>\n",
       "      <td>Aizawa 2019.pdf</td>\n",
       "      <td>55635726.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>30578081 Mäki-Nevala 2019.pdf</td>\n",
       "      <td>31398194 Badgujar 2019.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25669429 supp.doc</td>\n",
       "      <td>Kiesling 2014.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26573435 Díaz-Manera 2016.pdf</td>\n",
       "      <td>30696104 Caleca 2019.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26297796 supp table 1.pdf</td>\n",
       "      <td>Jiménez-Jáimez.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>25979592 Waldmüller 2015.pdf</td>\n",
       "      <td>24815523 Beckermann 2014.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28793149_Haugh_2017_Supp.pdf</td>\n",
       "      <td>burns 2019.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>31341521 Cortés 2019.pdf</td>\n",
       "      <td>28779003 Shen 2017.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8275</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>28339086 Chang 2017.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8276</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26808545 Kang 2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8277</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26327206 Gretarsdottir 2015.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8278</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>25962062 Han 2015.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8279</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27272900 Thouvenot 2016.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             supplementals              no_pmids  \\\n",
       "0                       24123850 supp.docx          Lim 2019.pdf   \n",
       "1      25590978_Inra_2015 supplemental.doc       Aizawa 2019.pdf   \n",
       "2                        25669429 supp.doc     Kiesling 2014.pdf   \n",
       "3                26297796 supp table 1.pdf  Jiménez-Jáimez.pdf   \n",
       "4             28793149_Haugh_2017_Supp.pdf        burns 2019.pdf   \n",
       "...                                    ...                   ...   \n",
       "8275                                  None                  None   \n",
       "8276                                  None                  None   \n",
       "8277                                  None                  None   \n",
       "8278                                  None                  None   \n",
       "8279                                  None                  None   \n",
       "\n",
       "          pmid_pdf preceeding_spaces  \\\n",
       "0     29365890.pdf              None   \n",
       "1     55635726.pdf              None   \n",
       "2             None              None   \n",
       "3             None              None   \n",
       "4             None              None   \n",
       "...            ...               ...   \n",
       "8275          None              None   \n",
       "8276          None              None   \n",
       "8277          None              None   \n",
       "8278          None              None   \n",
       "8279          None              None   \n",
       "\n",
       "                                                 others  \\\n",
       "0     25691505 related PhD thesis with carrier pedig...   \n",
       "1                        30578081 Mäki-Nevala 2019.pdf   \n",
       "2                        26573435 Díaz-Manera 2016.pdf   \n",
       "3                         25979592 Waldmüller 2015.pdf   \n",
       "4                             31341521 Cortés 2019.pdf   \n",
       "...                                                 ...   \n",
       "8275                                               None   \n",
       "8276                                               None   \n",
       "8277                                               None   \n",
       "8278                                               None   \n",
       "8279                                               None   \n",
       "\n",
       "                         correct_name  \n",
       "0          31894144 McKinney 2020.pdf  \n",
       "1          31398194 Badgujar 2019.pdf  \n",
       "2            30696104 Caleca 2019.pdf  \n",
       "3        24815523 Beckermann 2014.pdf  \n",
       "4              28779003 Shen 2017.pdf  \n",
       "...                               ...  \n",
       "8275          28339086 Chang 2017.pdf  \n",
       "8276           26808545 Kang 2016.pdf  \n",
       "8277  26327206 Gretarsdottir 2015.pdf  \n",
       "8278            25962062 Han 2015.pdf  \n",
       "8279      27272900 Thouvenot 2016.pdf  \n",
       "\n",
       "[8280 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF_STORE_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "loose-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_STORE_analysis.to_csv(DATAFILES/'pdf_store_analysis', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pmids = filter_results['no_pmids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "others = filter_results['others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cleanup.rename_pmid_files(\n",
    "    pmids, pmid_pubmed_data, delete=False, pmid_space_pdf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['renamed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_results['others'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pdfs = filter_results['pmid_pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup.remove_dups_with_parentheses(no_pmids, delete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup.convert_underscore_names(others, delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup.remove_preceeding_spaces(filter_results['preceeding_spaces'], delete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup.move_supplementals(filter_results['supplementals'], SUPP_DEST, remove=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup.rename_capitalized_extensions(check_file=True, rename=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_changes_df = pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_changes_df.to_csv('files_changed_3rd_run.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_dups_df = pd.DataFrame.from_dict(pmid_dups, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_dups_df.to_csv('pmid_duplicates.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_duplicates(list_of_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pmids = ['31894144', '28779003', '05181500', '16767104']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after query save data, as dataframe, to csv \n",
    "pmid_pubmed_data, errors = cleanup.get_pubmed_data(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df = pd.DataFrame.from_dict(pmid_pubmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df['pmid'] = pmid_pubmed_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df = pmid_pubmed_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df = pmid_pubmed_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df['pmid'] = pmid_pubmed_df['pmid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_df[pmid_pubmed_df['pmid'] == 10980535]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data = pd.read_csv(DATAFILES/'pmid_pubmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data[pmid_pubmed_data['pmid']==31600821]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to identify files that are duplicates by file size, \n",
    "# pick one to rename, save the putative duplicates to another\n",
    "# folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data[pmid_pubmed_data.eq('Scottish').any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_pubmed_data.replace('Scottish/Northern', 'Scottish-Northern', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-people",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
